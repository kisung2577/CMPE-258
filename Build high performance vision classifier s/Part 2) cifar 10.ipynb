{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["7MQjtEWif3Nr","HDZR8qjIiCe5","lq2U5ULwqYL4","ednp0QuRqrQk","RlQiy_-jts0a"],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["# Imports \n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_hub as hub\n","import tensorflow_datasets as tfds\n","from functools import partial\n","from tensorflow.keras.applications import EfficientNetB0"],"metadata":{"id":"gqx19f6kWTlC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n","    print(\"Device:\", tpu.master())\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","except ValueError:\n","    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n","    strategy = tf.distribute.MirroredStrategy()"],"metadata":{"id":"xkkOgnUkzrS3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fff61fc3-05c8-4410-e5f3-e00d2fd7bf7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Not connected to a TPU runtime. Using CPU/GPU strategy\n"]}]},{"cell_type":"code","source":["# Load data\n","\n","(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()"],"metadata":{"id":"Am_pfSyhau7I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"672cb19d-4780-4674-add8-b3fbc705aa92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 4s 0us/step\n"]}]},{"cell_type":"code","source":["# Split train into train and valid\n","X_train, X_valid = X_train_full[:-10000], X_train_full[-10000:]\n","y_train, y_valid = y_train_full[:-10000], y_train_full[-10000:]"],"metadata":{"id":"ukg8JkOPVxn2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.random.set_seed(42)"],"metadata":{"id":"YRnJTNUJiJBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.shape, X_valid.shape, X_test.shape, y_train.shape"],"metadata":{"id":"AfT1vAXnA24d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"27f80306-1bf1-4204-983d-e685b0c89fc8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((40000, 32, 32, 3), (10000, 32, 32, 3), (10000, 32, 32, 3), (40000, 1))"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["model = keras.models.Sequential()\n","model.add(keras.layers.Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(32, 32, 3)))\n","model.add(keras.layers.MaxPool2D(strides=2))\n","model.add(keras.layers.Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))\n","model.add(keras.layers.MaxPool2D(strides=2))\n","model.add(keras.layers.Flatten())\n","model.add(keras.layers.Dense(256, activation='relu'))\n","model.add(keras.layers.Dense(84, activation='relu'))\n","model.add(keras.layers.Dense(10, activation='softmax'))"],"metadata":{"id":"uALdJKoiqZ3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","history = model.fit(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sg-BL_KP798i","outputId":"7ada0662-9ace-4041-a96d-5085ef4d9616"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1250/1250 [==============================] - 8s 4ms/step - loss: 2.0509 - accuracy: 0.3023 - val_loss: 1.6206 - val_accuracy: 0.4245\n","Epoch 2/3\n","1250/1250 [==============================] - 7s 5ms/step - loss: 1.5070 - accuracy: 0.4631 - val_loss: 1.4041 - val_accuracy: 0.4972\n","Epoch 3/3\n","1250/1250 [==============================] - 5s 4ms/step - loss: 1.3534 - accuracy: 0.5222 - val_loss: 1.3887 - val_accuracy: 0.5214\n"]}]},{"cell_type":"code","source":["score = model.evaluate(X_test, y_test)\n","score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBoEY3Xy8KZk","outputId":"c17d8caa-cd9c-49e3-f654-4c51ee4c4910"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 2s 6ms/step - loss: 1.3874 - accuracy: 0.5202\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.3873687982559204, 0.5202000141143799]"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# Remove last layer\n","model = EfficientNetB0(include_top=False, weights='imagenet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTr6rdZXqsbQ","outputId":"5f72cb8c-b4e6-407a-adab-397d9853ff60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n","16705208/16705208 [==============================] - 2s 0us/step\n"]}]},{"cell_type":"code","source":["IMG_SIZE = 224\n","BATCH_SIZE = 64\n","NUM_CLASSES = 10"],"metadata":{"id":"yYipznm8zndg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create TensorFlow Datasets from the NumPy arrays\n","train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"],"metadata":{"id":"d2hvcUE3ByC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Resize image \n","\n","size = (IMG_SIZE, IMG_SIZE)\n","train_ds = train_ds.map(lambda image, label: (tf.image.resize(image, size), label))\n","test_ds = test_ds.map(lambda image, label: (tf.image.resize(image, size), label))"],"metadata":{"id":"uPQBTS2ngU5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# One-hot / categorical encoding\n","def input_preprocess(image, label):\n","    label = tf.one_hot(label, NUM_CLASSES)\n","    return image, label\n","\n","# Apply one-hot encoding to the training dataset\n","train_ds = train_ds.map(input_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n","train_ds = train_ds.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n","train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n","\n","# Apply one-hot encoding to the test dataset\n","test_ds = test_ds.map(input_preprocess)\n","test_ds = test_ds.batch(batch_size=BATCH_SIZE, drop_remainder=True)"],"metadata":{"id":"j553WlXZ0n9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_model(num_classes):\n","    inputs = keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n","    #x = img_augmentation(inputs)\n","\n","    # Transfer learning\n","    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n","    # Freeze the pretrained weights\n","    model.trainable = False\n","\n","    # Rebuild top\n","    x = keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n","    x = keras.layers.BatchNormalization()(x)\n","    top_dropout_rate = 0.2\n","    x = keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n","    outputs = keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"pred\")(x)\n","    outputs = tf.expand_dims(outputs, axis=1)\n","\n","    # Compile\n","    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    return model"],"metadata":{"id":"rGSzeQpd0ykB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = build_model(num_classes=NUM_CLASSES)\n","\n","hist = model.fit(train_ds, epochs=3, validation_data=test_ds, verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5co-Q6uz2VRP","outputId":"03d1e236-3da5-4eae-c347-1d14ac7e1380"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1, 10)\n","Epoch 1/3\n","625/625 - 93s - loss: 0.8027 - accuracy: 0.8245 - val_loss: 0.5214 - val_accuracy: 0.8677 - 93s/epoch - 148ms/step\n","Epoch 2/3\n","625/625 - 82s - loss: 0.4908 - accuracy: 0.8572 - val_loss: 0.3711 - val_accuracy: 0.8816 - 82s/epoch - 131ms/step\n","Epoch 3/3\n","625/625 - 81s - loss: 0.4195 - accuracy: 0.8645 - val_loss: 0.3892 - val_accuracy: 0.8756 - 81s/epoch - 130ms/step\n"]}]},{"cell_type":"code","source":["# Hyperparamters\n","\n","IMG_SIZE = 384\n","CROP_TO = 224\n","BATCH_SIZE = 64\n","STEPS_PER_EPOCH = 10\n","AUTO = tf.data.AUTOTUNE  \n","NUM_CLASSES = 10\n","SCHEDULE_LENGTH = (20)\n","SCHEDULE_BOUNDARIES = [5, 10, 15]"],"metadata":{"id":"hNi5IzMJvfRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create TensorFlow Datasets from the NumPy arrays\n","\n","train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"],"metadata":{"id":"Hyn9LYG5YArA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data preprocessing functions\n","\n","@tf.function\n","def preprocess_train(image, label):\n","    image = tf.image.random_flip_left_right(image)\n","    image = tf.image.resize(image, (CROP_TO, CROP_TO))\n","    image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))\n","    image = image / 255.0\n","    return (image, label)\n","\n","@tf.function\n","def preprocess_test(image, label):\n","    image = tf.image.resize(image, (CROP_TO, CROP_TO))\n","    image = image / 255.0\n","    return (image, label)\n","\n","DATASET_NUM_TRAIN_EXAMPLES = train_ds.cardinality().numpy()\n","\n","repeat_count = int(SCHEDULE_LENGTH * BATCH_SIZE / DATASET_NUM_TRAIN_EXAMPLES * STEPS_PER_EPOCH)\n","repeat_count += 10 + 1"],"metadata":{"id":"nmAgOochvxHJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocess the training data pipeline\n","train_ds = train_ds.shuffle(10000)\n","train_ds = train_ds.repeat(repeat_count)\n","train_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTO)\n","train_ds = train_ds.batch(BATCH_SIZE)\n","train_ds = train_ds.prefetch(AUTO)\n","\n","# Preprocess the test data pipeline\n","test_ds = test_ds.map(preprocess_test, num_parallel_calls=AUTO)\n","test_ds = test_ds.batch(BATCH_SIZE)\n","test_ds = test_ds.prefetch(AUTO)"],"metadata":{"id":"Sj8g-L_Wv_H7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load pretrained model\n","\n","bit_model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n","bit_module = hub.KerasLayer(bit_model_url)"],"metadata":{"id":"eVkMZOP6wIR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BiTModel(keras.Model):\n","  def __init__(self, num_classes, module, **kwargs):\n","    super().__init__(**kwargs)\n","    self.num_classes = num_classes\n","    self.head = keras.layers.Dense(num_classes, kernel_initializer=\"zeros\")\n","    self.bit_model = module\n","\n","  def call(self, images):\n","    bit_embedding = self.bit_model(images)\n","    return self.head(bit_embedding)\n","\n","model = BiTModel(num_classes=NUM_CLASSES, module=bit_module)"],"metadata":{"id":"kCktCSZxwcSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optimizer and loss\n","\n","learning_rate = 0.005 * BATCH_SIZE / 512\n","\n","# Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\n","lr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(\n","    boundaries=SCHEDULE_BOUNDARIES,\n","    values=[\n","        learning_rate,\n","        learning_rate * 0.1,\n","        learning_rate * 0.01,\n","        learning_rate * 0.001,\n","    ],\n",")\n","optimizer = keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n","\n","loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"],"metadata":{"id":"DIsEeedEwt5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])"],"metadata":{"id":"ihSiXOo6w8oN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Early stopping callback\n","\n","callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=2, restore_best_weights=True)]"],"metadata":{"id":"b0xJOxT8w_mP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    train_ds,\n","    batch_size=BATCH_SIZE,\n","    epochs=3,\n","    steps_per_epoch=STEPS_PER_EPOCH,\n","    validation_data=test_ds,\n","    callbacks=callbacks,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xwAgvZdxHdI","outputId":"aa81cfe1-c94d-4f97-e50a-04b59fc31faf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","10/10 [==============================] - 86s 10s/step - loss: 0.8217 - accuracy: 0.7000 - val_loss: 0.6816 - val_accuracy: 0.7553\n","Epoch 2/3\n","10/10 [==============================] - 86s 10s/step - loss: 0.6968 - accuracy: 0.7563 - val_loss: 0.6228 - val_accuracy: 0.7829\n","Epoch 3/3\n","10/10 [==============================] - 86s 9s/step - loss: 0.5830 - accuracy: 0.7969 - val_loss: 0.6009 - val_accuracy: 0.7918\n"]}]},{"cell_type":"code","source":["# Evaluate model\n","\n","accuracy = model.evaluate(test_ds)[1] * 100\n","print(\"Accuracy: {:.2f}%\".format(accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R6UR0XNlxZ33","outputId":"e53294e7-5713-4a76-e05a-06a8f90b4a36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["157/157 [==============================] - 61s 389ms/step - loss: 0.6009 - accuracy: 0.7918\n","Accuracy: 79.18%\n"]}]},{"cell_type":"code","source":["num_classes = 10\n","input_shape = (32, 32, 3)\n","\n","weight_decay = 0.0001\n","batch_size = 128\n","num_epochs = 3\n","dropout_rate = 0.2\n","image_size = 64  \n","patch_size = 8  \n","num_patches = (image_size // patch_size) ** 2 \n","embedding_dim = 256  \n","num_blocks = 4  \n","learning_rate = 0.005"],"metadata":{"id":"5Vjo5Na74j7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Patches(keras.layers.Layer):\n","  def __init__(self, patch_size, num_patches):\n","    super().__init__()\n","    self.patch_size = patch_size\n","    self.num_patches = num_patches\n","\n","  def call(self, images):\n","    batch_size = tf.shape(images)[0]\n","    patches = tf.image.extract_patches(\n","        images=images,\n","        sizes=[1, self.patch_size, self.patch_size, 1],\n","        strides=[1, self.patch_size, self.patch_size, 1],\n","        rates=[1, 1, 1, 1],\n","        padding=\"VALID\",\n","    )\n","    patch_dims = patches.shape[-1]\n","    patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n","    return patches"],"metadata":{"id":"O7LXdPu173kI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_augmentation = keras.Sequential(\n","    [\n","        keras.layers.Normalization(),\n","        keras.layers.Resizing(image_size, image_size),\n","        keras.layers.RandomFlip(\"horizontal\"),\n","        keras.layers.RandomZoom(\n","            height_factor=0.2, width_factor=0.2\n","        ),\n","    ],\n","    name=\"data_augmentation\",\n",")\n","data_augmentation.layers[0].adapt(X_train)"],"metadata":{"id":"WHaOJW3t9pVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_classifier(blocks, positional_encoding=False):\n","    inputs = keras.layers.Input(shape=input_shape)\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    patches = Patches(patch_size, num_patches)(augmented)\n","    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n","    x = keras.layers.Dense(units=embedding_dim)(patches)\n","    if positional_encoding:\n","        positions = tf.range(start=0, limit=num_patches, delta=1)\n","        position_embedding = keras.layers.Embedding(\n","            input_dim=num_patches, output_dim=embedding_dim\n","        )(positions)\n","        x = x + position_embedding\n","\n","    # Process x using the module blocks.\n","    x = blocks(x)\n","\n","    representation = keras.layers.GlobalAveragePooling1D()(x)\n","    representation = keras.layers.Dropout(rate=dropout_rate)(representation)\n","    logits = keras.layers.Dense(num_classes)(representation)\n","\n","    return keras.Model(inputs=inputs, outputs=logits)"],"metadata":{"id":"ITVv5sKt7TAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_experiment(model):\n","    # Create Adam optimizer with weight decay.\n","    optimizer = keras.optimizers.AdamW(\n","        learning_rate=learning_rate, weight_decay=weight_decay,\n","    )\n","    # Compile the model.\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[\n","            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n","            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n","        ],\n","    )\n","    # Create a learning rate scheduler callback.\n","    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n","        monitor=\"val_loss\", factor=0.5, patience=5\n","    )\n","    # Create an early stopping callback.\n","    early_stopping = tf.keras.callbacks.EarlyStopping(\n","        monitor=\"val_loss\", patience=10, restore_best_weights=True\n","    )\n","    # Fit the model.\n","    history = model.fit(\n","        x=X_train,\n","        y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_split=0.1,\n","        callbacks=[early_stopping, reduce_lr],\n","    )\n","\n","    _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","\n","    # Return history to plot learning curves.\n","    return history"],"metadata":{"id":"-PNAgQAW8I_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MLPMixerLayer(keras.layers.Layer):\n","    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","\n","        self.mlp1 = keras.Sequential(\n","            [\n","                keras.layers.Dense(units=num_patches),\n","                #tfa.layers.GELU(),\n","                keras.layers.Dense(units=num_patches),\n","                keras.layers.Dropout(rate=dropout_rate),\n","            ]\n","        )\n","        self.mlp2 = keras.Sequential(\n","            [\n","                keras.layers.Dense(units=num_patches),\n","                #tfa.layers.GELU(),\n","                keras.layers.Dense(units=embedding_dim),\n","                keras.layers.Dropout(rate=dropout_rate),\n","            ]\n","        )\n","        self.normalize = keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    def call(self, inputs):\n","        # Apply layer normalization.\n","        x = self.normalize(inputs)\n","        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n","        x_channels = tf.linalg.matrix_transpose(x)\n","        # Apply mlp1 on each channel independently.\n","        mlp1_outputs = self.mlp1(x_channels)\n","        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n","        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n","        # Add skip connection.\n","        x = mlp1_outputs + inputs\n","        # Apply layer normalization.\n","        x_patches = self.normalize(x)\n","        # Apply mlp2 on each patch independtenly.\n","        mlp2_outputs = self.mlp2(x_patches)\n","        # Add skip connection.\n","        x = x + mlp2_outputs\n","        return x"],"metadata":{"id":"LKO2nANg4nIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlpmixer_blocks = keras.Sequential(\n","    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",")\n","\n","mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n","history = run_experiment(mlpmixer_classifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_me3vfQ5tfk","outputId":"ed84ffcd-d8fd-402d-c071-811498c0c0af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","282/282 [==============================] - 25s 50ms/step - loss: 1.9354 - acc: 0.3034 - top5-acc: 0.7991 - val_loss: 1.8861 - val_acc: 0.3580 - val_top5-acc: 0.8550 - lr: 0.0050\n","Epoch 2/3\n","282/282 [==============================] - 13s 45ms/step - loss: 1.6546 - acc: 0.4022 - top5-acc: 0.8839 - val_loss: 1.5963 - val_acc: 0.4313 - val_top5-acc: 0.8985 - lr: 0.0050\n","Epoch 3/3\n","282/282 [==============================] - 13s 45ms/step - loss: 1.5717 - acc: 0.4321 - top5-acc: 0.9001 - val_loss: 1.7317 - val_acc: 0.4308 - val_top5-acc: 0.8863 - lr: 0.0050\n","313/313 [==============================] - 4s 11ms/step - loss: 1.6728 - acc: 0.4317 - top5-acc: 0.8995\n","Test accuracy: 43.17%\n","Test top 5 accuracy: 89.95%\n"]}]}]}